{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Installing RDKit</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "import deepchem as dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/Users/misspotato/Documents/Github/Binding-Free-Energy-Prediction-Host-Guest-System/Results/host-guest-dataset_clean.xlsx')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data randomly\n",
    "np.random.shuffle(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>dataset group name</th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Host</th>\n",
       "      <th>Guest</th>\n",
       "      <th>Ex _G_(kcal/mol)</th>\n",
       "      <th>Ex _G_SEM</th>\n",
       "      <th>EX _H_(kcal/mol)</th>\n",
       "      <th>EX _H_SEM</th>\n",
       "      <th>pb_guest_Etot</th>\n",
       "      <th>...</th>\n",
       "      <th>gb_host_1-4EEL</th>\n",
       "      <th>gb_host_EELEC</th>\n",
       "      <th>gb_host_EGB</th>\n",
       "      <th>gb_host_ESURF</th>\n",
       "      <th>gb_delta_H</th>\n",
       "      <th>pb_delta_H</th>\n",
       "      <th>EX _delta_H_(kcal/mol)</th>\n",
       "      <th>gb_Ex_difference</th>\n",
       "      <th>SQR_gbnsr6_Ex_difference</th>\n",
       "      <th>pb_Ex_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>Mobley benchmarkset</td>\n",
       "      <td>cd-set1</td>\n",
       "      <td>acd</td>\n",
       "      <td>guest-11</td>\n",
       "      <td>-2.720</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-3.28</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-51.7020</td>\n",
       "      <td>...</td>\n",
       "      <td>422.5394</td>\n",
       "      <td>-418.6250</td>\n",
       "      <td>-97.5622</td>\n",
       "      <td>5.1074</td>\n",
       "      <td>-6.8615</td>\n",
       "      <td>-5.0642</td>\n",
       "      <td>-3.28</td>\n",
       "      <td>-3.5815</td>\n",
       "      <td>12.827142</td>\n",
       "      <td>-1.7842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>Mobley benchmarkset</td>\n",
       "      <td>cd-set1</td>\n",
       "      <td>acd</td>\n",
       "      <td>guest-17</td>\n",
       "      <td>-3.227</td>\n",
       "      <td>1.135</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-5.0152</td>\n",
       "      <td>...</td>\n",
       "      <td>423.2458</td>\n",
       "      <td>-437.2679</td>\n",
       "      <td>-91.6246</td>\n",
       "      <td>5.0761</td>\n",
       "      <td>-11.4839</td>\n",
       "      <td>-11.8241</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-10.5539</td>\n",
       "      <td>111.384805</td>\n",
       "      <td>-10.8941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>Mobley benchmarkset</td>\n",
       "      <td>cd-set2</td>\n",
       "      <td>bcd</td>\n",
       "      <td>guest-14</td>\n",
       "      <td>-1.554</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.1436</td>\n",
       "      <td>...</td>\n",
       "      <td>497.2683</td>\n",
       "      <td>-495.0822</td>\n",
       "      <td>-121.3069</td>\n",
       "      <td>5.9335</td>\n",
       "      <td>-4.8565</td>\n",
       "      <td>-5.9462</td>\n",
       "      <td>0.88</td>\n",
       "      <td>-5.7365</td>\n",
       "      <td>32.907432</td>\n",
       "      <td>-6.8262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>Mobley benchmarkset</td>\n",
       "      <td>cd-set2</td>\n",
       "      <td>bcd</td>\n",
       "      <td>guest-10</td>\n",
       "      <td>-2.192</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-2.89</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-47.3442</td>\n",
       "      <td>...</td>\n",
       "      <td>488.4410</td>\n",
       "      <td>-486.0020</td>\n",
       "      <td>-117.8558</td>\n",
       "      <td>5.9297</td>\n",
       "      <td>-11.7835</td>\n",
       "      <td>-11.7177</td>\n",
       "      <td>-2.89</td>\n",
       "      <td>-8.8935</td>\n",
       "      <td>79.094342</td>\n",
       "      <td>-8.8277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>Mobley benchmarkset</td>\n",
       "      <td>cd-set2</td>\n",
       "      <td>bcd</td>\n",
       "      <td>guest-15</td>\n",
       "      <td>-4.175</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-2.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-6.1893</td>\n",
       "      <td>...</td>\n",
       "      <td>500.9283</td>\n",
       "      <td>-493.6747</td>\n",
       "      <td>-122.1523</td>\n",
       "      <td>5.9483</td>\n",
       "      <td>-13.8003</td>\n",
       "      <td>-12.9928</td>\n",
       "      <td>-2.29</td>\n",
       "      <td>-11.5103</td>\n",
       "      <td>132.487006</td>\n",
       "      <td>-10.7028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>8</td>\n",
       "      <td>Mobley benchmarkset</td>\n",
       "      <td>cd-set1</td>\n",
       "      <td>acd</td>\n",
       "      <td>guest-8</td>\n",
       "      <td>-4.622</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-4.89</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-70.0470</td>\n",
       "      <td>...</td>\n",
       "      <td>418.4633</td>\n",
       "      <td>-421.1437</td>\n",
       "      <td>-95.0137</td>\n",
       "      <td>5.1211</td>\n",
       "      <td>-15.8181</td>\n",
       "      <td>-14.5525</td>\n",
       "      <td>-4.89</td>\n",
       "      <td>-10.9281</td>\n",
       "      <td>119.423370</td>\n",
       "      <td>-9.6625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>56</td>\n",
       "      <td>SAMPL5</td>\n",
       "      <td>sample</td>\n",
       "      <td>OAH</td>\n",
       "      <td>guest-3</td>\n",
       "      <td>-5.060</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-5.93</td>\n",
       "      <td>0.08</td>\n",
       "      <td>46.0430</td>\n",
       "      <td>...</td>\n",
       "      <td>-82.4983</td>\n",
       "      <td>678.2864</td>\n",
       "      <td>-1388.0243</td>\n",
       "      <td>8.3998</td>\n",
       "      <td>-21.2137</td>\n",
       "      <td>-17.7786</td>\n",
       "      <td>-5.93</td>\n",
       "      <td>-15.2837</td>\n",
       "      <td>233.591486</td>\n",
       "      <td>-11.8486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>61</td>\n",
       "      <td>SAMPL5</td>\n",
       "      <td>sample</td>\n",
       "      <td>OAME</td>\n",
       "      <td>guest-2</td>\n",
       "      <td>-5.040</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-7.56</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-88.3150</td>\n",
       "      <td>...</td>\n",
       "      <td>350.2523</td>\n",
       "      <td>385.2185</td>\n",
       "      <td>-1350.3593</td>\n",
       "      <td>8.6313</td>\n",
       "      <td>-2.3246</td>\n",
       "      <td>-0.0453</td>\n",
       "      <td>-7.56</td>\n",
       "      <td>5.2354</td>\n",
       "      <td>27.409413</td>\n",
       "      <td>7.5147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>55</td>\n",
       "      <td>SAMPL5</td>\n",
       "      <td>sample</td>\n",
       "      <td>OAH</td>\n",
       "      <td>guest-2</td>\n",
       "      <td>-4.250</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-89.4726</td>\n",
       "      <td>...</td>\n",
       "      <td>-82.6409</td>\n",
       "      <td>675.4077</td>\n",
       "      <td>-1390.4029</td>\n",
       "      <td>8.3838</td>\n",
       "      <td>-9.2216</td>\n",
       "      <td>-7.4032</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-4.8316</td>\n",
       "      <td>23.344359</td>\n",
       "      <td>-3.0132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>38</td>\n",
       "      <td>Mobley benchmarkset</td>\n",
       "      <td>cd-set2</td>\n",
       "      <td>bcd</td>\n",
       "      <td>guest-16</td>\n",
       "      <td>-4.319</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-2.27</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-19.3196</td>\n",
       "      <td>...</td>\n",
       "      <td>504.8576</td>\n",
       "      <td>-518.2366</td>\n",
       "      <td>-105.0833</td>\n",
       "      <td>5.8828</td>\n",
       "      <td>-12.6354</td>\n",
       "      <td>-10.8424</td>\n",
       "      <td>-2.27</td>\n",
       "      <td>-10.3654</td>\n",
       "      <td>107.441517</td>\n",
       "      <td>-8.5724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID   dataset group name Dataset Name  Host     Guest  Ex _G_(kcal/mol)  \\\n",
       "0   11  Mobley benchmarkset      cd-set1   acd  guest-11            -2.720   \n",
       "1   17  Mobley benchmarkset      cd-set1   acd  guest-17            -3.227   \n",
       "2   36  Mobley benchmarkset      cd-set2   bcd  guest-14            -1.554   \n",
       "3   32  Mobley benchmarkset      cd-set2   bcd  guest-10            -2.192   \n",
       "4   37  Mobley benchmarkset      cd-set2   bcd  guest-15            -4.175   \n",
       "..  ..                  ...          ...   ...       ...               ...   \n",
       "57   8  Mobley benchmarkset      cd-set1   acd   guest-8            -4.622   \n",
       "58  56               SAMPL5       sample   OAH   guest-3            -5.060   \n",
       "59  61               SAMPL5       sample  OAME   guest-2            -5.040   \n",
       "60  55               SAMPL5       sample   OAH   guest-2            -4.250   \n",
       "61  38  Mobley benchmarkset      cd-set2   bcd  guest-16            -4.319   \n",
       "\n",
       "    Ex _G_SEM  EX _H_(kcal/mol)  EX _H_SEM  pb_guest_Etot  ...  \\\n",
       "0       0.004             -3.28       0.02       -51.7020  ...   \n",
       "1       1.135             -0.93       0.32        -5.0152  ...   \n",
       "2       0.167              0.88       0.17         0.1436  ...   \n",
       "3       0.013             -2.89       0.05       -47.3442  ...   \n",
       "4       0.010             -2.29       0.03        -6.1893  ...   \n",
       "..        ...               ...        ...            ...  ...   \n",
       "57      0.017             -4.89       0.03       -70.0470  ...   \n",
       "58      0.010             -5.93       0.08        46.0430  ...   \n",
       "59      0.030             -7.56       0.10       -88.3150  ...   \n",
       "60      0.010             -4.39       0.03       -89.4726  ...   \n",
       "61      0.006             -2.27       0.01       -19.3196  ...   \n",
       "\n",
       "    gb_host_1-4EEL  gb_host_EELEC  gb_host_EGB  gb_host_ESURF  gb_delta_H  \\\n",
       "0         422.5394      -418.6250     -97.5622         5.1074     -6.8615   \n",
       "1         423.2458      -437.2679     -91.6246         5.0761    -11.4839   \n",
       "2         497.2683      -495.0822    -121.3069         5.9335     -4.8565   \n",
       "3         488.4410      -486.0020    -117.8558         5.9297    -11.7835   \n",
       "4         500.9283      -493.6747    -122.1523         5.9483    -13.8003   \n",
       "..             ...            ...          ...            ...         ...   \n",
       "57        418.4633      -421.1437     -95.0137         5.1211    -15.8181   \n",
       "58        -82.4983       678.2864   -1388.0243         8.3998    -21.2137   \n",
       "59        350.2523       385.2185   -1350.3593         8.6313     -2.3246   \n",
       "60        -82.6409       675.4077   -1390.4029         8.3838     -9.2216   \n",
       "61        504.8576      -518.2366    -105.0833         5.8828    -12.6354   \n",
       "\n",
       "    pb_delta_H  EX _delta_H_(kcal/mol)  gb_Ex_difference  \\\n",
       "0      -5.0642                   -3.28           -3.5815   \n",
       "1     -11.8241                   -0.93          -10.5539   \n",
       "2      -5.9462                    0.88           -5.7365   \n",
       "3     -11.7177                   -2.89           -8.8935   \n",
       "4     -12.9928                   -2.29          -11.5103   \n",
       "..         ...                     ...               ...   \n",
       "57    -14.5525                   -4.89          -10.9281   \n",
       "58    -17.7786                   -5.93          -15.2837   \n",
       "59     -0.0453                   -7.56            5.2354   \n",
       "60     -7.4032                   -4.39           -4.8316   \n",
       "61    -10.8424                   -2.27          -10.3654   \n",
       "\n",
       "    SQR_gbnsr6_Ex_difference  pb_Ex_difference  \n",
       "0                  12.827142           -1.7842  \n",
       "1                 111.384805          -10.8941  \n",
       "2                  32.907432           -6.8262  \n",
       "3                  79.094342           -8.8277  \n",
       "4                 132.487006          -10.7028  \n",
       "..                       ...               ...  \n",
       "57                119.423370           -9.6625  \n",
       "58                233.591486          -11.8486  \n",
       "59                 27.409413            7.5147  \n",
       "60                 23.344359           -3.0132  \n",
       "61                107.441517           -8.5724  \n",
       "\n",
       "[62 rows x 45 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(frac = 1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Reading Mobley PDB files</h1>\n",
    "<p>Here each PDB file will be read and saved in Mol data type defined in RDKit and used by DeepChem</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with complex names as keys and molecule as values\n",
    "PDBs = {}\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = '/Users/misspotato/Documents/Github/Binding-Free-Energy-Prediction-Host-Guest-System/PDB'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "for f in onlyfiles:\n",
    "    PDBs.update({f.split('.')[0].replace('-s', '') : rdkit.Chem.rdmolfiles.MolFromPDBFile(mypath + '/' + f)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDBs.pop('',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Featurizing</h1>\n",
    "<p>GraphConv model needs ConvMolFeaturizer</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = dc.feat.ConvMolFeaturizer(per_atom_fragmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "X_ids = []\n",
    "one_add = 0 if len(PDBs.keys()) % 2 == 0 else 1\n",
    "for k in PDBs.keys():\n",
    "    X_ids.append(k)\n",
    "    X.append(featurizer.featurize(PDBs[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "X_ids = []\n",
    "one_add = 0 if len(PDBs.keys()) % 2 == 0 else 1\n",
    "for k in PDBs.keys():\n",
    "    X_ids.append(k)\n",
    "    X.append(featurizer.featurize(PDBs[k]))\n",
    "X = [x[0] for x in X]\n",
    "X_train_featurized = X[:int(len(X) / 2)]\n",
    "X_test_featurized = X[int(len(X) / 2) + one_add:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_names = [i.split('-')[0] for i in X_ids]\n",
    "guest_names = ['guest-' + (i.split('-')[1].replace('s', '')) for i in X_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_names_train = host_names[:int(len(host_names) / 2)]\n",
    "guest_names_train = guest_names[:int(len(guest_names) / 2)]\n",
    "host_names_test = host_names[int(len(host_names) / 2) + one_add:]\n",
    "guest_names_test = guest_names[int(len(guest_names) / 2) + one_add:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_add_train, x_add_test, y_train, y_test = [], [], [], []\n",
    "for i in range(len(host_names_train)):\n",
    "    new_df = df[(df['Host'] == host_names_train[i]) & (df['Guest'] == guest_names_train[i])]\n",
    "    y_train.append(new_df['EX _H_(kcal/mol)'].to_numpy()[0])\n",
    "    x_add_train.append(new_df[[c for c in df.columns if ('Etot' not in c) and ('delta' not in c)\n",
    "                         and ('Ex_difference' not in c) and ('gb_' in c or 'VDWAALS' in c)]].to_numpy()[0])\n",
    "y_train = np.array(y_train)\n",
    "    \n",
    "for i in range(len(host_names_test)):\n",
    "    new_df = df[(df['Host'] == host_names_test[i]) & (df['Guest'] == guest_names_test[i])]\n",
    "    y_test.append(new_df['EX _H_(kcal/mol)'].to_numpy()[0])\n",
    "    x_add_test.append(new_df[[c for c in df.columns if ('Etot' not in c) and ('delta' not in c)\n",
    "                         and ('Ex_difference' not in c) and ('gb_' in c or 'VDWAALS' in c)]].to_numpy()[0])\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.metrics import to_one_hot\n",
    "from deepchem.feat.mol_graphs import ConvMol\n",
    "\n",
    "x_preprocessed_train, x_preprocessed_test = [], []\n",
    "\n",
    "## for X train\n",
    "multiConvMol = ConvMol.agglomerate_mols(X_train_featurized)\n",
    "x_preprocessed_train = [multiConvMol.get_atom_features(), multiConvMol.deg_slice, np.array(multiConvMol.membership)]\n",
    "for i in range(1, len(multiConvMol.get_deg_adjacency_lists())):\n",
    "    x_preprocessed_train.append(multiConvMol.get_deg_adjacency_lists()[i])\n",
    "x_preprocessed_train.append(np.array(x_add_train))\n",
    "\n",
    "## for X test\n",
    "multiConvMol = ConvMol.agglomerate_mols(X_test_featurized)\n",
    "x_preprocessed_test = [multiConvMol.get_atom_features(), multiConvMol.deg_slice, np.array(multiConvMol.membership)]\n",
    "for i in range(1, len(multiConvMol.get_deg_adjacency_lists())):\n",
    "    x_preprocessed_test.append(multiConvMol.get_deg_adjacency_lists()[i])\n",
    "x_preprocessed_test.append(np.array(x_add_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.full([14, np.max([v.shape[0] for v in x_preprocessed_train]),\n",
    "                  np.max([v.shape[1] for v in x_preprocessed_train if len(v.shape) > 1])], 1.123456)\n",
    "for i,j in enumerate(x_preprocessed_train):\n",
    "    if len(j.shape) > 1:\n",
    "        x_train[i][:j.shape[0],:j.shape[1]] = np.array(j)\n",
    "    else:\n",
    "        x_train[i][:len(j), :1] = np.array(j).reshape(j.shape[0], 1)\n",
    "x_train = x_train.reshape([1] + list(x_train.shape))\n",
    "\n",
    "x_test = np.full([14, np.max([v.shape[0] for v in x_preprocessed_test]),\n",
    "                  np.max([v.shape[1] for v in x_preprocessed_test if len(v.shape) > 1])], 1.123456)\n",
    "for i,j in enumerate(x_preprocessed_test):\n",
    "    if len(j.shape) > 1:\n",
    "        x_test[i][:j.shape[0],:j.shape[1]] = np.array(j)\n",
    "    else:\n",
    "        x_test[i][:len(j), :1] = np.array(j).reshape(j.shape[0], 1)\n",
    "x_test = x_test.reshape([1] + list(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 14, 2833, 75)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 14, 3251, 75)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Creating Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_features = []\n",
    "# for x_feat in X:\n",
    "#     multiConvMol = ConvMol.agglomerate_mols([x_feat])\n",
    "#     new_x_feat = [multiConvMol.get_atom_features(), multiConvMol.deg_slice, np.array(multiConvMol.membership)]\n",
    "#     for i in range(1, len(multiConvMol.get_deg_adjacency_lists())):\n",
    "#         new_x_feat.append(multiConvMol.get_deg_adjacency_lists()[i])\n",
    "#     X_features.append(new_x_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !!!!!!!! important\n",
    "## !!!!!!!! important\n",
    "## !!!!!!!! important\n",
    "## !!!!!!!! important\n",
    "input_shapes = [i.shape for i in x_preprocessed_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2833, 75),\n",
       " (11, 2),\n",
       " (2833,),\n",
       " (649, 1),\n",
       " (981, 2),\n",
       " (1203, 3),\n",
       " (0, 4),\n",
       " (0, 5),\n",
       " (0, 6),\n",
       " (0, 7),\n",
       " (0, 8),\n",
       " (0, 9),\n",
       " (0, 10),\n",
       " (31, 15)]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.models.layers import GraphConv, GraphPool, GraphGather\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.layers import Dense, Input, BatchNormalization, Concatenate\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "batch_size = int(len(df) / 2)\n",
    "\n",
    "class GBGraphConvModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(GBGraphConvModel, self).__init__()\n",
    "    self.gc1 = GraphConv(64, activation_fn=tf.nn.tanh)\n",
    "    self.batch_norm1 = layers.BatchNormalization()\n",
    "    self.gp1 = GraphPool()\n",
    "\n",
    "    self.gc2 = GraphConv(64, activation_fn=tf.nn.tanh)\n",
    "    self.batch_norm2 = layers.BatchNormalization()\n",
    "    self.gp2 = GraphPool()\n",
    "\n",
    "    self.dense1 = layers.Dense(128, activation=tf.nn.tanh)\n",
    "    self.batch_norm3 = layers.BatchNormalization()\n",
    "    self.readout = GraphGather(batch_size=batch_size, activation_fn=tf.nn.tanh)\n",
    "\n",
    "    self.dense2 = layers.Dense(1)\n",
    "    self.dense3 = layers.Dense(1, \n",
    "         kernel_initializer=initializers.Constant([.5, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1]),\n",
    "         bias_initializer=initializers.Zeros())\n",
    "\n",
    "  def call(self, inputs):\n",
    "    inputs = inputs[0]\n",
    "    x = []\n",
    "#     input_shapes = [[4822, 75], [11, 2], [4822], [1142, 1], [1635, 2], [2042, 3],\n",
    "#                    [3, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9], [0, 10]]\n",
    "    for i in range(len(input_shapes)):\n",
    "        x.append(tf.reshape(inputs[i][inputs[i] != 1.123456], input_shapes[i]))\n",
    "    for i in range(1, len(input_shapes)):\n",
    "        x[i] = tf.cast(x[i], tf.int32)\n",
    "    x_add = tf.reshape(inputs[13][inputs[13] != 1.123456], [batch_size, 15])\n",
    "    gc1_output = self.gc1(x)\n",
    "    batch_norm1_output = self.batch_norm1(gc1_output)\n",
    "    gp1_output = self.gp1([batch_norm1_output] + x[1:])\n",
    "\n",
    "    gc2_output = self.gc2([gp1_output] + x[1:])\n",
    "    batch_norm2_output = self.batch_norm1(gc2_output)\n",
    "    gp2_output = self.gp2([batch_norm2_output] + x[1:])\n",
    "\n",
    "    dense1_output = self.dense1(gp2_output)\n",
    "    batch_norm3_output = self.batch_norm3(dense1_output)\n",
    "    readout_output = self.readout([batch_norm3_output] + x[1:])\n",
    "\n",
    "    model_var = self.dense2(readout_output)\n",
    "    binding_affinity = tf.concat([model_var, x_add], axis=1)\n",
    "    return self.dense3(binding_affinity)\n",
    "model = GBGraphConvModel()\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/misspotato/opt/miniconda3/envs/rdkit-deepchem-jupyter/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_pool_9/Reshape_11:0\", shape=(649,), dtype=int32), values=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_pool_9/Reshape_10:0\", shape=(649, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_pool_9/Cast_3:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/misspotato/opt/miniconda3/envs/rdkit-deepchem-jupyter/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_pool_9/Reshape_14:0\", shape=(1962,), dtype=int32), values=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_pool_9/Reshape_13:0\", shape=(1962, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_pool_9/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/misspotato/opt/miniconda3/envs/rdkit-deepchem-jupyter/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_pool_9/Reshape_17:0\", shape=(3609,), dtype=int32), values=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_pool_9/Reshape_16:0\", shape=(3609, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_pool_9/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/misspotato/opt/miniconda3/envs/rdkit-deepchem-jupyter/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_11:0\", shape=(649,), dtype=int32), values=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_10:0\", shape=(649, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/misspotato/opt/miniconda3/envs/rdkit-deepchem-jupyter/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_13:0\", shape=(1962,), dtype=int32), values=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_12:0\", shape=(1962, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Cast_1:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/misspotato/opt/miniconda3/envs/rdkit-deepchem-jupyter/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_15:0\", shape=(3609,), dtype=int32), values=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_14:0\", shape=(3609, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Cast_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/misspotato/opt/miniconda3/envs/rdkit-deepchem-jupyter/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_17:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_16:0\", shape=(0, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Cast_3:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/misspotato/opt/miniconda3/envs/rdkit-deepchem-jupyter/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_19:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_18:0\", shape=(0, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/misspotato/opt/miniconda3/envs/rdkit-deepchem-jupyter/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_21:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_20:0\", shape=(0, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/misspotato/opt/miniconda3/envs/rdkit-deepchem-jupyter/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_23:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_22:0\", shape=(0, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Cast_6:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/misspotato/opt/miniconda3/envs/rdkit-deepchem-jupyter/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_25:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_24:0\", shape=(0, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Cast_7:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/misspotato/opt/miniconda3/envs/rdkit-deepchem-jupyter/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_27:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_26:0\", shape=(0, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Cast_8:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/misspotato/opt/miniconda3/envs/rdkit-deepchem-jupyter/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_29:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Reshape_28:0\", shape=(0, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_conv_9/Cast_9:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/misspotato/opt/miniconda3/envs/rdkit-deepchem-jupyter/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_pool_8/Reshape_11:0\", shape=(649,), dtype=int32), values=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_pool_8/Reshape_10:0\", shape=(649, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_pool_8/Cast_3:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/misspotato/opt/miniconda3/envs/rdkit-deepchem-jupyter/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_pool_8/Reshape_14:0\", shape=(1962,), dtype=int32), values=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_pool_8/Reshape_13:0\", shape=(1962, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_pool_8/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/misspotato/opt/miniconda3/envs/rdkit-deepchem-jupyter/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_pool_8/Reshape_17:0\", shape=(3609,), dtype=int32), values=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_pool_8/Reshape_16:0\", shape=(3609, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/gb_graph_conv_model_4/graph_pool_8/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step - loss: 78.2213\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 49.6618\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 32.5092\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 24.3505\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 22.8390\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 25.4951\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 29.8017\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 32.4366\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 32.4312\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 30.6171\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 27.9032\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 24.4089\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 21.8228\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 20.7812\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 20.4567\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 20.7572\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 21.3387\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 21.9736\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 22.4952\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 22.3097\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 21.6933\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 20.7143\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 19.6772\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 18.9213\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 18.4506\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 18.3327\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 18.4266\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 18.5964\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 18.9076\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 18.9889\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 18.7095\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 18.2628\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17.7830\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17.4540\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 17.1656\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17.0167\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 16.8540\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 16.7963\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 16.8580\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 16.8403\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 16.6257\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 16.4962\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 16.2180\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 15.8835\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 15.8173\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 15.7166\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 15.6337\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 15.5625\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 15.3767\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 15.1962\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 15.0869\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 14.9418\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 14.7450\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 14.6593\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 14.5684\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 14.4607\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 14.3512\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 14.2322\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 14.1608\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 14.0688\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 13.9437\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 13.8736\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 13.7653\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 13.6335\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 13.5472\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 13.4167\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 13.3430\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 13.2467\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 13.1340\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 13.0474\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 12.9327\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 12.8972\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 12.8100\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 12.7005\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 12.5546\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 12.5252\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 12.5704\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 12.4528\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 12.3957\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 12.3410\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 12.1865\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 12.2082\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 12.0895\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 12.0124\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 11.8901\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 11.8611\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 11.7943\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 11.7358\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 11.6541\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 11.6008\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 11.5110\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 11.4506\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 11.3534\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 11.2486\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 11.1709\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 11.1557\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 11.1204\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 11.0762\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 11.0136\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 10.9291\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 10.8574\n",
      "Epoch 102/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step - loss: 10.8133\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 10.7420\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 10.6714\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 10.6095\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 10.5545\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 10.4960\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 10.4491\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 10.4186\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 10.4217\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 10.4036\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 10.3828\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 10.3487\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 10.2985\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 10.2706\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 10.2355\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 10.1814\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 10.1370\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 10.1206\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 10.0450\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 10.0329\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 9.9986\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9.9476\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9.9429\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.8896\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.8603\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 9.8308\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9.8059\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 9.7723\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9.7461\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.7122\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.6889\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 9.6363\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 9.6480\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9.6266\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9.6049\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9.5796\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 9.5576\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.5402\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.5162\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.4988\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 9.4774\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 9.4572\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 9.4358\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 9.4168\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9.3996\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.3838\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 9.3678\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 9.3497\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.3330\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 9.3148\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 9.2944\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9.2724\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 9.2484\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 9.2304\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.2157\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 9.1946\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 9.1759\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.1583\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 9.1403\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 9.1236\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 9.1050\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 9.0785\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 9.0646\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9.0478\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 9.0343\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 9.0186\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9.0072\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.9913\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.9796\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.9675\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.9531\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.9448\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.9323\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.9234\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.9115\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.9029\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.8887\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.8836\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.8670\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.8560\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.8526\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.8584\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.8555\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.8462\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.8391\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.8268\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.8148\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.8071\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.7948\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.7895\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.7796\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.7650\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.7628\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.7677\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.7671\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.7751\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.7729\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.7610\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.7549\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.7405\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.7287\n",
      "Epoch 203/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step - loss: 8.7216\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.6845\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.7265\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.6706\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.6926\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.7234\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.7044\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.6743\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.6576\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.6421\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.6224\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.6345\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.6049\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.6520\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.6344\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.6231\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.6401\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.6075\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.5835\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.5695\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.5628\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.5753\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.5714\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.6310\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 8.5990\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.5772\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.5912\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.5710\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.5715\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.5811\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.5552\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.5510\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.5439\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.5264\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.5137\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.5140\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.5038\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.4887\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.4752\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.4801\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.4645\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.4571\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.4574\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.4448\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.4374\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.4356\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.4275\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.4229\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.4199\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.4133\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.4094\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.4029\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.3986\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.3954\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.3900\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.3850\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.3806\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.3759\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.3715\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.3675\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.3627\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.3588\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.3545\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.3497\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.3452\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.3392\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.3272\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.3239\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.3212\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.3160\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.3112\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.3060\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.3028\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 8.2990\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.2958\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.2920\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.2886\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.2851\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.2819\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.2773\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.2707\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 8.2685\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.2641\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.2618\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.2596\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.2560\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.2530\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.2508\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.2487\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.2458\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.2432\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.2408\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.2388\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.2365\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.2341\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.2319\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.2301\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.2280\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.2260\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.2239\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.2221\n",
      "Epoch 304/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step - loss: 8.2202\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.2183\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.2164\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.2147\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.2127\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.2109\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.2090\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.2074\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.2057\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 8.2041\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 8.2025\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.2009\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.1990\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.1974\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.1958\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.1941\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 8.1925\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.1911\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 8.1895\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.1880\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.1866\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.1850\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.1834\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.1820\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 8.1806\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.1792\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.1777\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.1763\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.1750\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.1734\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.1720\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.1706\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.1693\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.1679\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.1665\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.1652\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.1639\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.1625\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.1613\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.1600\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.1587\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.1574\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.1561\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.1549\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 8.1536\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.1521\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.1510\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.1497\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.1484\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 8.1471\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.1460\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.1447\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.1435\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 8.1423\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 8.1411\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8.1399\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.1386\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.1374\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 8.1361\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.1351\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.1340\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.1328\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.1316\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.1304\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.1292\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.1280\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.1269\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.1258\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.1245\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.1234\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.1223\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.1210\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.1198\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.1187\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 8.1175\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.1162\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.1150\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.1138\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.1127\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.1114\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.1102\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.1091\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.1079\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.1068\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.1058\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.1047\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.1038\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.1029\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.1018\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.1008\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.0998\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.0989\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.0980\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.0970\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.0960\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.0952\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.0944\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.0935\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.0926\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.0917\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.0909\n",
      "Epoch 405/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step - loss: 8.0901\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.0891\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.0883\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 8.0874\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.0865\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.0857\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.0850\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.0843\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.0835\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 8.0827\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.0820\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.0812\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.0805\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.0798\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.0792\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.0784\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.0776\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.0769\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.0763\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.0755\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.0749\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8.0742\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.0735\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 8.0729\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.0722\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.0715\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.0708\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.0702\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.0695\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.0689\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 8.0683\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.0677\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.0670\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 8.0664\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.0658\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 8.0652\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.0646\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.0639\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.0632\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.0626\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.0619\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.0613\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.0608\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 8.0602\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 8.0595\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.0589\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.0582\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.0578\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.0572\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.0564\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.0560\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.0553\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.0547\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.0541\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.0535\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.0531\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.0524\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.0518\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 8.0513\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.0508\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 8.0502\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.0497\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.0492\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 8.0486\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.0481\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.0476\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.0470\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.0462\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.0457\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.0452\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.0448\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.0443\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.0437\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.0432\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.0426\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 8.0421\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.0413\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.0407\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.0403\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.0398\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.0393\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 8.0387\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.0381\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.0376\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.0371\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.0366\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.0360\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 8.0356\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.0350\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.0346\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.0342\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.0337\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.0331\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.0326\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.0320\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.0298\n"
     ]
    }
   ],
   "source": [
    "train_history = model.fit(x_train, y_train.reshape([1, -1]), epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shapes = [i.shape for i in x_preprocessed_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3251, 75),\n",
       " (11, 2),\n",
       " (3251,),\n",
       " (679, 1),\n",
       " (1189, 2),\n",
       " (1377, 3),\n",
       " (6, 4),\n",
       " (0, 5),\n",
       " (0, 6),\n",
       " (0, 7),\n",
       " (0, 8),\n",
       " (0, 9),\n",
       " (0, 10),\n",
       " (31, 15)]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_test_function.<locals>.test_function at 0x187cdec10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 1s/step - loss: 34.6240\n"
     ]
    }
   ],
   "source": [
    "test_history = model.evaluate(x_test, y_test.reshape([1, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.884216175498653"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(34.6240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Physics based model RMSE </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "train_sum=0\n",
    "for i in range(len(host_names_train)):\n",
    "    new_df = df[(df['Host'] == host_names_train[i]) & (df['Guest'] == guest_names_train[i])]\n",
    "    train_sum += new_df['gb_Ex_difference'].to_numpy()[0] **2\n",
    "\n",
    "\n",
    "test_sum = 0\n",
    "for i in range(len(host_names_test)):\n",
    "    new_df = df[(df['Host'] == host_names_test[i]) & (df['Guest'] == guest_names_test[i])]\n",
    "    test_sum += new_df['gb_Ex_difference'].to_numpy()[0] **2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_based_rmse_train = math.sqrt(train_sum / len(host_names_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_based_rmse_test = math.sqrt((test_sum) / len(host_names_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE on training set is : {physics_based_rmse_train}\")\n",
    "print(f\"rmse on testing set is : {physics_based_rmse_test}\")\n",
    "# Total rmse\n",
    "total_rmse_physics = np.sqrt(np.mean((df['EX _H_(kcal/mol)'].to_numpy() - df['gb_delta_H'].to_numpy())**2))\n",
    "print(f\"RMSE of the total data: {total_rmse_physics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Loss per epoch </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.callbacks.History object at 0x187be37c0>\n"
     ]
    }
   ],
   "source": [
    "print(train_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Cross Validation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkit-deepchem-jupyter",
   "language": "python",
   "name": "rdkit-deepchem-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
